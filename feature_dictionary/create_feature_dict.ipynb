{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!pip install av <-- 설치하고 세션 재시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import VivitImageProcessor\n",
    "\n",
    "image_processor = VivitImageProcessor.from_pretrained(\"kkumtori/vivit-b-16x2-kinetics400-finetuned-0505-mediapipe\")\n",
    "\n",
    "video_cls = pipeline(model=\"kkumtori/vivit-b-16x2-kinetics400-finetuned-0505-mediapipe\")\n",
    "video_cls.image_processor = image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import av\n",
    "import torch\n",
    "from transformers import VivitImageProcessor, VivitForVideoClassification\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "def process_video_files(folder_path):\n",
    "    image_processor = video_cls.image_processor\n",
    "    model = video_cls.model\n",
    "    feature_dict = defaultdict(list)\n",
    "\n",
    "    # 폴더명을 클래스 이름으로 사용\n",
    "    for class_label in os.listdir(folder_path):\n",
    "        class_path = os.path.join(folder_path, class_label)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        # 클래스 별 폴더 내 모든 파일을 탐색\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith(\".mp4\"):\n",
    "                file_path = os.path.join(class_path, filename)\n",
    "                container = av.open(file_path)\n",
    "\n",
    "                # 32 프레임 샘플링\n",
    "                indices = sample_frame_indices(clip_len=32, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "                video = read_video_pyav(container=container, indices=indices)\n",
    "\n",
    "                # 비디오를 모델에 맞게 준비\n",
    "                inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "                # 모델을 통한 전파\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs, output_hidden_states=True)\n",
    "                    logits = outputs.logits\n",
    "                    hidden_states = outputs.hidden_states\n",
    "                    last_hidden = hidden_states[-1]\n",
    "\n",
    "                # 클래스별로 마지막 히든 레이어의 특징 저장\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                for idx, prediction in enumerate(predictions):\n",
    "                    feature_dict[class_label].append(last_hidden[idx].numpy())\n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "# 폴더 경로 설정 및 함수 호출\n",
    "folder_path = '/content/drive/MyDrive/기컴비_텀프/data/train_dataset/mediapipe/train'\n",
    "all_features = process_video_files(folder_path)\n",
    "print(all_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def max_pooling_features(feature_dict):\n",
    "    max_pooled_features = {}\n",
    "    for class_label, features_list in feature_dict.items():\n",
    "        features_array = np.array(features_list)\n",
    "        max_pooled_feature = np.max(features_array, axis=0)\n",
    "        max_pooled_features[class_label] = max_pooled_feature\n",
    "    return max_pooled_features\n",
    "\n",
    "# process_video_files()의 output: feature_dict\n",
    "# max_pooling_features()의 input: feature_dict\n",
    "max_pooled_features = max_pooling_features(all_features)\n",
    "print(max_pooled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(max_pooled_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 파일로 저장\n",
    "with open(\"/content/drive/MyDrive/max_pooled_features.pkl\", 'wb') as f:\n",
    "    pickle.dump(max_pooled_features, f)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/all_features.pkl\", 'wb') as f:\n",
    "    pickle.dump(all_features, f)\n",
    "\n",
    "# 파일 불러오기\n",
    "# with open(\"/content/drive/MyDrive/max_pooled_features.pkl\", 'rb') as f:\n",
    "#     loaded_dict = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
