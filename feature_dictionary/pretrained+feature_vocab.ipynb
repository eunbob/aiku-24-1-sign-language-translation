{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install av"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-fjclkfps-5","executionInfo":{"status":"ok","timestamp":1715174788857,"user_tz":-540,"elapsed":8934,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"5240ba55-2766-484a-c209-50bbd0cacdc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting av\n","  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: av\n","Successfully installed av-12.0.0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2dG7C0PqWgN","executionInfo":{"status":"ok","timestamp":1715174672961,"user_tz":-540,"elapsed":18382,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"cc8cbc1b-279f-41f1-ebf5-0c3eab87033f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import av\n","import numpy as np\n","import torch\n","from transformers import VivitImageProcessor, VivitForVideoClassification\n","from collections import defaultdict"],"metadata":{"id":"OXrRNtiTz4xJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(0)\n","\n","def read_video_pyav(container, indices):\n","    frames = []\n","    container.seek(0)\n","    start_index = indices[0]\n","    end_index = indices[-1]\n","    for i, frame in enumerate(container.decode(video=0)):\n","        if i > end_index:\n","            break\n","        if i >= start_index and i in indices:\n","            frames.append(frame)\n","    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n","\n","def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n","    converted_len = int(clip_len * frame_sample_rate)\n","    end_idx = np.random.randint(converted_len, seg_len)\n","    start_idx = end_idx - converted_len\n","    indices = np.linspace(start_idx, end_idx, num=clip_len)\n","    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n","    return indices\n","\n","def process_video_files(folder_path):\n","    # 모델 및 프로세서 초기화\n","    from transformers import pipeline\n","\n","    image_processor = VivitImageProcessor.from_pretrained(\"kkumtori/vivit-b-16x2-kinetics400-finetuned-0505-mediapipe\")\n","    model = pipeline(\"kkumtori/vivit-b-16x2-kinetics400-finetuned-0505-mediapipe\")\n","    model.imageprocessor = image_processor\n","\n","    feature_dict = defaultdict(list)\n","\n","    # 폴더 내 모든 파일을 탐색\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".mp4\"):  # 비디오 파일 형식 필터링\n","            file_path = os.path.join(folder_path, filename)\n","            container = av.open(file_path)\n","\n","            # 32 프레임 샘플링\n","            indices = sample_frame_indices(clip_len=32, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n","            video = read_video_pyav(container=container, indices=indices)\n","\n","            # 비디오를 모델에 맞게 준비\n","            inputs = image_processor(list(video), return_tensors=\"pt\")\n","\n","            # 모델을 통한 전파\n","            with torch.no_grad():\n","                outputs = model(**inputs, output_hidden_states=True)\n","                logits = outputs.logits\n","                hidden_states = outputs.hidden_states\n","                last_hidden = hidden_states[-1]\n","\n","            # 클래스별로 마지막 히든 레이어의 특징 저장\n","            predictions = torch.argmax(logits, dim=-1)\n","            for idx, prediction in enumerate(predictions):\n","                feature_dict[prediction.item()].append(last_hidden[idx].numpy())\n","\n","    return feature_dict\n","\n","# 폴더 경로 설정 및 함수 호출\n","#folder_path = '/content/drive/MyDrive/temp'\n","folder_path = '/content/drive/MyDrive/기컴비_텀프/data/temp'\n","all_features = process_video_files(folder_path)\n","print(all_features)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y3HUWpRBzumN","executionInfo":{"status":"ok","timestamp":1715174992986,"user_tz":-540,"elapsed":29616,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"ef9eecff-37e6-4049-ac25-8b628fced845"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["defaultdict(<class 'list'>, {0: [array([[-7.9534988e+00, -4.3826671e+00,  3.4476194e+00, ...,\n","        -4.7930422e+00, -7.5185823e+00,  3.0407934e+00],\n","       [-1.6844152e+01, -3.0463426e+00,  4.3826237e+00, ...,\n","        -1.2342141e+00,  8.0614477e-02, -6.6756001e+00],\n","       [-6.3012667e+00,  3.2564924e+00,  2.4743316e+00, ...,\n","        -7.5133653e+00, -1.4035599e+00, -1.8838451e+01],\n","       ...,\n","       [-1.6991383e+01, -2.7283037e+00,  7.3920298e+00, ...,\n","         2.4419069e-01, -2.1484270e+00, -7.0126429e+00],\n","       [-1.6931517e+01, -2.1895020e+00,  6.9496880e+00, ...,\n","        -1.2084007e-02, -2.4033704e+00, -7.1066146e+00],\n","       [-1.6302195e+01, -3.4971147e+00,  8.7466049e+00, ...,\n","         5.2495599e-01, -1.2600405e+00, -7.2345486e+00]], dtype=float32)]})\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Wu85zGx3Yecl"},"execution_count":null,"outputs":[]}]}